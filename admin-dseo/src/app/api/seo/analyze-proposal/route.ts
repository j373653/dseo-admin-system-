import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL || ''
const supabaseServiceKey = process.env.SUPABASE_SERVICE_KEY || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY || ''

const GOOGLE_API_URL = 'https://generativelanguage.googleapis.com/v1beta/models'
const OPENROUTER_API_URL = 'https://openrouter.ai/api/v1'

async function getAIConfig() {
  const supabase = createClient(supabaseUrl, supabaseServiceKey)
  
  const { data } = await supabase
    .from('d_seo_admin_ai_config')
    .select('task, model, parameters, active')
  
  if (!data) return { silo: { model: 'gemini-2.5-pro', parameters: { maxTokens: 20000, temperature: 0.3 } } }
  
  const config: { [key: string]: any } = {}
  for (const item of data) {
    if (item.active !== false) {
      config[item.task] = {
        model: item.model,
        parameters: item.parameters || {}
      }
    }
  }
  return config
}

async function getCompanyContext() {
  const supabase = createClient(supabaseUrl, supabaseServiceKey)
  
  const { data } = await supabase
    .from('d_seo_admin_company_context')
    .select('key, value')
  
  if (!data) return null
  
  const context: { [key: string]: any } = {}
  for (const item of data) {
    try {
      context[item.key] = typeof item.value === 'string' ? JSON.parse(item.value) : item.value
    } catch (e) {
      context[item.key] = item.value
    }
  }
  return context
}

function slugify(text: string): string {
  return (text || '')
    .toString()
    .toLowerCase()
    .normalize('NFD')
    .replace(/[\u0300-\u036f]/g, '')
    .replace(/[^a-z0-9]+/g, '-')
    .replace(/^-+|-+$/g, '')
}

interface SiloProposal {
  name: string
  categories: {
    name: string
    pages: {
      main_keyword_id: string
      main_keyword?: string
      secondary_keywords_ids: string[]
      secondary_keywords?: string[]
      type: 'service' | 'blog' | 'landing'
      is_pillar: boolean
      intent: 'informational' | 'transactional' | 'commercial'
    }[]
  }[]
}

interface SiloForPrompt {
  name: string
  categories: {
    name: string
    pages: {
      main_keyword: string
    }[]
  }[]
}

async function analyzeSilosWithGemini(
  keywords: { id: string; keyword: string }[],
  apiKey: string,
  existingSilos: SiloForPrompt[] = [],
  context: { theme?: string; services?: string[]; target_companies?: string[]; sitemap_urls?: string[]; discard_topics?: string[] } = {},
  aiModel: string = 'gemini-2.5-pro',
  aiParams: { maxTokens?: number; temperature?: number } = {},
  attempt: number = 1
): Promise<{ silos: SiloProposal[]; intentions: { [key: string]: string }; validationErrors: string[] }> {
  const MAX_RETRIES = 2
  
  // Create a map for quick ID lookup
  const keywordMap = new Map(keywords.map(k => [k.keyword.toLowerCase().trim(), k.id]))
  
  // Format keywords with IDs for the prompt
  const keywordList = keywords.map((k, i) => `ID:${k.id} â†’ "${k.keyword}"`).join('\n')
  
  const existingSilosBlock = existingSilos.length > 0
    ? `EXISTING SILOS (para referencia - NO necesariamente usar estos):\n${
        existingSilos.map(s => 
          `Silo: ${s.name}\n` +
          s.categories.map(c => 
            `  CategorÃ­a: ${c.name}\n` +
            c.pages.map(p => `    PÃ¡gina: ${p.main_keyword}`).join('\n')
          ).join('\n')
        ).join('\n\n')
      }\n`
    : ''

  const theme = context.theme || 'Desarrollo Web, SEO, Marketing Digital, Apps, IA'
  const services = context.services?.join(', ') || 'Desarrollo web, WordPress, Ecommerce, SEO, IA, Apps'
  const targetCompanies = context.target_companies?.join(', ') || 'PYMEs, AutÃ³nomos, Startups'
  const sitemapUrls = context.sitemap_urls?.map(u => u.replace('https://d-seo.es', '')).join('\n') || ''
  const discardTopics = context.discard_topics?.join(', ') || 'redes sociales, ads, facebook, instagram, hosting, dominios'

  const companyContext = `CONTEXTO DE LA EMPRESA (d-seo.es):
- TEMA PRINCIPAL: ${theme}
- SERVICIOS: ${services}
- CLIENTES OBJETIVO: ${targetCompanies}
- TEMÃTICAS A DESCARTAR (NO trabajar con estas): ${discardTopics}

SITEMAP ACTUAL DEL SITIO WEB (evitar duplicar contenido existente):
${sitemapUrls || 'Sin sitemap disponible'}`

  // Lista de pilares vÃ¡lidos predefinidos (estructura consolidada a 4 pilares)
  const validSilos = `PILARES VÃLIDOS Y OBLIGATORIOS (usar estos 4 pilares existentes):
- Pilar 1: Desarrollo Web & E-commerce (incluye: WordPress, Tiendas Online, Mantenimiento, Legal, DiseÃ±o Web)
- Pilar 2: Aplicaciones & Software (incluye: Apps iOS/Android, Apps Escritorio, PWAs, Desarrollo a Medida)
- Pilar 3: IA & Automatizaciones (incluye: Chatbots, Automatizaciones, Agentes IA, IA para Empresas)
- Pilar 4: SEO & Marketing Digital (incluye: SEO TÃ©cnico, SEO Local, SEO por Sectores, AnalÃ­tica, AuditorÃ­as)

CATEGORÃAS ESENCIALES POR PILAR (usar solo estas categorÃ­as - NO crear nuevas):
- Pilar 1: DiseÃ±o Web, WordPress, Ecommerce, Mantenimiento, Textos Legales, Desarrollo Web
- Pilar 2: Apps MÃ³viles, Apps Escritorio, PWAs, Desarrollo a Medida
- Pilar 3: Chatbots, Automatizaciones, Agentes IA, IA para Empresas
- Pilar 4: SEO TÃ©cnico, SEO Local, SEO por Sectores, AnalÃ­tica, AuditorÃ­as

INSTRUCCIONES DE PILARES:
- USA los 4 pilares de arriba EXACTAMENTE como estÃ¡n escritos
- NO crees nuevos pilares - usa uno de los 4 listados
- NO crees nuevas categorÃ­as - usa solo las listadas arriba
- Si una keyword no encaja â†’ usar la categorÃ­a mÃ¡s cercana de la lista
- Evita TODA duplicaciÃ³n de categorÃ­as`

  const prompt = `### ROL: Senior SEO Strategist & Information Architect
ActÃºa como un consultor SEO experto con 15 aÃ±os de experiencia en arquitectura de informaciÃ³n y jerarquÃ­a de contenidos. Tu especialidad es la creaciÃ³n de estructuras SILO que maximizan el traspaso de autoridad y evitan la canibalizaciÃ³n.

### CONTEXTO Y OBJETIVO
El objetivo es diseÃ±ar una PROPUESTA DE ESTRUCTURA SILO lÃ³gica y optimizada utilizando ÃšNICAMENTE una lista de palabras clave proporcionada.

### DATOS DE ENTRADA
1. PALABRAS CLAVE A ANALIZAR (SOLO estas, NO inventes):
${keywordList}

2. ESTRUCTURA ACTUAL (para referencia):
${existingSilosBlock || 'Sin estructura existente'}

3. TEMAS A DESCARTAR: ${discardTopics}

### REGLAS CRÃTICAS DE CUMPLIMIENTO
1. PROHIBICIÃ“N DE INVENCIÃ“N: No puedes aÃ±adir, modificar ni inventar ninguna keyword. Si no estÃ¡ en la lista "PALABRAS CLAVE A ANALIZAR", NO EXISTE.
2. Las secondary_keywords DEBEN ser keywords de la lista proporcionada
3. FILTRO NEGATIVO: Descarta inmediatamente cualquier keyword que coincida con: ${discardTopics}
4. ANTI-CANIBALIZACIÃ“N: No proposes pÃ¡ginas que ya existen en el sitemap actual
5. Cada pÃ¡gina propuesta debe tener una main_keyword Ãºnica de la lista proporcionada
6. USA LOS 4 PILARES DEFINIDOS - NO creas pilares nuevos

### FASES DE EJECUCIÃ“N
1. Fase de Limpieza: Filtra la lista original eliminando temas descartados
2. Fase de Clustering: Agrupa las keywords por intenciÃ³n de bÃºsqueda y temÃ¡tica
3. Fase de Arquitectura: Asigna cada keyword a un SILO existente y crea categorÃ­as dentro si es necesario

### ESTRUCTURA PILAR - USAR SOLO LOS 4 PILARES DEFINIDOS
${validSilos}

Para CADA pÃ¡gina, especifica:
- main_keyword: Keyword principal (OBLIGATORIO: debe estar en la lista de PALABRAS CLAVE A ANALIZAR)
- secondary_keywords: Keywords secundarias relacionadas (de la lista proporcionada)
- type: "service" (servicio), "blog" (artÃ­culo), "landing" (aterrizaje)
- is_pillar: true si es la pÃ¡gina mÃ¡s importante de la categorÃ­a
- intent: "informational" (informaciÃ³n), "transactional" (compra/contratar), "commercial" (compara)

### REGLAS FINALES - ANTI-CANIBALIZACIÃ“N ESTRICTA
- NO INVENTES keywords - usa EXACTAMENTE las de la lista
- NO CREES NUEVOS PILARES - usa solo los 4 pilares definidos arriba
- NO CREES NUEVAS CATEGORÃAS - usa solo las categorÃ­as listadas arriba
- keywords "transactional" â†’ tipo "service" o "landing"
- keywords "informational" â†’ tipo "blog"

### REGLA CRÃTICA: UNA KEYWORD = UNA PÃGINA
- CADA keyword debe aparecer EXACTAMENTE UNA VEZ en toda la propuesta
- NO repitas keywords en mÃºltiples pÃ¡ginas (ni como main ni como secondary)
- Si una keyword similar ya fue asignada, NO la asignes de nuevo
- Es MEJOR tener muchas secondary_keywords en UNA pÃ¡gina que la misma keyword en VARIAS pÃ¡ginas
- Las keywords muy similares (misma raÃ­z, plural/singular, variaciones) deben IR JUNTAS en la misma pÃ¡gina

### EJEMPLO DE GROUPING DE KEYWORDS SIMILARES (OBLIGATORIO SEGUIR):
**MAL (canibalizaciÃ³n):**
- "crear pagina web con ia" â†’ pÃ¡gina 1
- "crea pagina web con ia" â†’ pÃ¡gina 2
- "crear pÃ¡ginas web con ia" â†’ pÃ¡gina 3

**BIEN (una pÃ¡gina para todas):**
- "crear pagina web con ia", "crea pagina web con ia", "crear pÃ¡ginas web con ia" â†’ UNA SOLA pÃ¡gina con main_keyword="crear pagina web con ia" y secondary_keywords=["crea pagina web con ia", "crear pÃ¡ginas web con ia", ...]

**LISTA DE VARIACIONES COMUNES A AGRUPAR:**
- crear/crea/creo/creado
- pagina/pÃ¡ginas/pagina web/paginas web
- web/sitio web/sitio
- con ia/con inteligencia artificial
- hacer/haz/realiza
- una/--
- con/sin

**NORMA: Si dos keywords comparten â‰¥3 palabras significativas, van a la MISMA pÃ¡gina.**

### ðŸš¨ REGLA OBLIGATORIA: TODAS LAS KEYWORDS
- **DEBES incluir TODAS las keywords de la lista de entrada en la propuesta**
- NO puedes ignorar ninguna keyword
- Si una keyword no encaja en ninguna categorÃ­a â†’ asÃ­gnala a la categorÃ­a mÃ¡s genÃ©rica
- Si una keyword NO es relevante para el negocio â†’ inclÃºyela en una pÃ¡gina existente como secondary_keyword (no crees pÃ¡ginas solo para descartar)
- **Keywords que la IA ignora serÃ¡n marcadas como error**

### FORMATO DE SALIDA JSON:
- Para cada pÃ¡gina, incluye TANTO el ID como el texto de la keyword (para validaciÃ³n cruzada)
- Formato: "main_keyword_id": "uuid-aqui", "main_keyword": "texto de la keyword"
- secondary_keywords_ids: array de IDs, secondary_keywords: array de textos
${`{
  "silos": [
    {
      "name": "Desarrollo Web",
      "categories": [
        {
          "name": "WordPress",
          "pages": [
            {
              "main_keyword_id": "uuid-aqui",
              "main_keyword": "desarrollo wordpress madrid",
              "secondary_keywords_ids": ["uuid-1", "uuid-2"],
              "secondary_keywords": ["keyword sec 1", "keyword sec 2"],
              "type": "service",
              "is_pillar": true,
              "intent": "transactional"
            }
          ]
        }
      ]
    }
  ],
  "intentions": {}
}`}
            {
              "main_keyword": "cuÃ¡nto cuesta una web wordpress",
              "secondary_keywords": ["precio web wordpress", "presupuesto wordpress"],
              "type": "blog",
              "is_pillar": false,
              "intent": "commercial"
            }
          ]
        }
      ]
    }
  ],
  "intentions": {
    "keyword_exacta": "informational|transactional|commercial"
  }
}`
  
  try {

  // Configurar thinking solo para Flash (Pro no soporta disable)
  const generationConfig: any = {
    temperature: aiParams.temperature ?? 0.3,
    maxOutputTokens: aiParams.maxTokens ?? 20000,
    responseMimeType: 'application/json'
  }
  
  // Solo Flash puede usar thinking_budget: 0
  if (aiModel.includes('flash')) {
    generationConfig.thinkingConfig = {
      thinkingBudget: 0
    }
  }

  // Determinar si usar OpenRouter o Google
  // OpenRouter models: openai/*, openrouter/*, provider/model:free, etc.
  const isOpenRouter = aiModel.includes('openai/') || aiModel.includes('openrouter/') || aiModel.includes('/') || aiModel.includes(':free')
  const openrouterApiKey = process.env.OPENROUTER_API_KEY
  
  let response
  let content
  
  if (isOpenRouter) {
    // OpenRouter API - modelos ya tienen prefijo como "openai/", "arcee-ai/", etc.
    const modelName = aiModel
    
    const requestBody = {
      model: modelName,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: aiParams.maxTokens ?? 20000,
      temperature: aiParams.temperature ?? 0.3,
      response_format: { type: 'json_object' }
    }

    response = await fetch(
      `${OPENROUTER_API_URL}/chat/completions`,
      {
        method: 'POST',
        headers: { 
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${openrouterApiKey}`,
          'HTTP-Referer': 'https://admin.d-seo.es',
          'X-Title': 'D-SEO Admin'
        },
        body: JSON.stringify(requestBody),
        signal: AbortSignal.timeout(300000)
      }
    )

    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`OpenRouter error ${response.status}: ${errorText}`)
    }

    const data = await response.json()
    
    if (!data.choices || !data.choices[0]?.message?.content) {
      throw new Error('Respuesta invÃ¡lida de OpenRouter')
    }

    content = data.choices[0].message.content
  } else {
    // Google Gemini API (original)
    const requestBody = {
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig
    }

    response = await fetch(
      `${GOOGLE_API_URL}/${aiModel}:generateContent?key=${apiKey}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(requestBody),
        signal: AbortSignal.timeout(300000)
      }
    )

    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`Gemini error ${response.status}: ${errorText}`)
    }

    const data = await response.json()
    
    if (!data.candidates || !data.candidates[0]?.content?.parts[0]?.text) {
      throw new Error('Respuesta invÃ¡lida de Gemini')
    }

    content = data.candidates[0].content.parts[0].text
  }

    // FunciÃ³n robusta para extraer JSON
    function extractJSON(text: string): any {
      // MÃ©todo 1: Buscar bloques JSON completos con ```
      const codeBlockMatch = text.match(/```(?:json)?\s*([\s\S]*?)```/)
      if (codeBlockMatch) {
        try {
          return JSON.parse(codeBlockMatch[1].trim())
        } catch (e) { /* continue */ }
      }
      
      // MÃ©todo 2: Buscar desde primer { hasta Ãºltimo }
      const braceMatch = text.match(/\{[\s\S]*\}/)
      if (braceMatch) {
        try {
          return JSON.parse(braceMatch[0])
        } catch (e) { /* continue */ }
      }
      
      // MÃ©todo 3: Intentar reparar JSON truncado de forma mÃ¡s robusta
      const partialMatch = text.match(/\{[\s\S]+/)
      if (partialMatch) {
        let fixed = partialMatch[0]
        
        // Cerrar corchetes abiertos
        const bracketOpens = (fixed.match(/\[/g) || []).length
        const bracketCloses = (fixed.match(/\]/g) || []).length
        if (bracketOpens > bracketCloses) {
          fixed += ']'.repeat(bracketOpens - bracketCloses)
        }
        
        // Cerrar llaves abiertas
        const braceOpens = (fixed.match(/\{/g) || []).length
        const braceCloses = (fixed.match(/\}/g) || []).length
        if (braceOpens > braceCloses) {
          fixed += '}'.repeat(braceOpens - braceCloses)
        }
        
        // Eliminar comillas abiertas al final
        fixed = fixed.replace(/,\s*$/, '')
        fixed = fixed.replace(/"$/, '')
        
        try {
          return JSON.parse(fixed)
        } catch (e) {
          // Ãšltimo intento: buscar solo la parte vÃ¡lida del JSON
          const validStart = fixed.indexOf('{"silos"')
          if (validStart >= 0) {
            const partialJson = fixed.substring(validStart)
            try {
              return JSON.parse(partialJson)
            } catch (e2) { /* fall through */ }
          }
        }
      }
      
      return null
    }

    let parsed: any
    try {
      parsed = extractJSON(content)
      if (!parsed) {
        throw new Error('No se pudo extraer JSON vÃ¡lido de la respuesta')
      }
    } catch (parseError: any) {
      console.error('JSON Parse error. Content:', content.slice(0, 1000))
      throw new Error('Error parseando JSON de Gemini: ' + content.slice(0, 200))
    }

    if (!parsed.silos) {
      throw new Error('Respuesta sin estructura de silos')
    }

    // ValidaciÃ³n post-IA: verificar que los IDs de keywords existen en la lista original
    const validKeywordIds = new Set(keywords.map(k => k.id))
    const validationErrors: string[] = []
    
    // FunciÃ³n para validar IDs y convertirlos a formato de salida
    const validateAndConvert = (silos: SiloProposal[]): { silos: any[]; intentions: { [key: string]: string } } => {
      const convertedSilos = []
      const intentions: { [key: string]: string } = {}
      
      for (const silo of silos) {
        if (!silo || !silo.name) continue
        const convertedCategories = []
        for (const cat of (silo.categories || [])) {
          if (!cat || !cat.name) continue
          const convertedPages = []
          for (const page of (cat.pages || [])) {
            // OPCIÃ“N A: Matching hÃ­brido - intentar ID primero, luego por texto
            let keywordId = page.main_keyword_id
            let mainKeywordText = page.main_keyword ? String(page.main_keyword) : ''
            
            // Si el ID no es vÃ¡lido, buscar por texto (matching flexible)
            if (!keywordId || !validKeywordIds.has(keywordId)) {
              if (mainKeywordText && mainKeywordText.trim()) {
                const mainTextLower = mainKeywordText.toLowerCase().trim()
                // Primero buscar coincidencia exacta
                let match = keywords.find(k => 
                  k.keyword.toLowerCase().trim() === mainTextLower
                )
                // Si no hay coincidencia exacta, buscar por inclusiÃ³n
                if (!match) {
                  match = keywords.find(k => 
                    k.keyword.toLowerCase().includes(mainTextLower) ||
                    mainTextLower.includes(k.keyword.toLowerCase())
                  )
                }
                if (match) {
                  keywordId = match.id
                  console.log(`Main keyword match por texto: "${mainKeywordText}" â†’ ID: ${keywordId}`)
                } else {
                  console.log(`Main keyword no encontrada: "${mainKeywordText}"`)
                  validationErrors.push(`Keyword "${mainKeywordText}" no encontrada en la lista`)
                  continue
                }
              } else {
                validationErrors.push(`Main keyword ID "${keywordId}" no vÃ¡lido`)
                continue
              }
            }
            
            // Validar secondary keywords IDs - tambiÃ©n intentar matching por texto
            const secondaryIds: string[] = []
            const secondaryTexts = page.secondary_keywords ? (Array.isArray(page.secondary_keywords) ? page.secondary_keywords : [page.secondary_keywords]) : []
            const secondaryIdTexts = page.secondary_keywords_ids ? (Array.isArray(page.secondary_keywords_ids) ? page.secondary_keywords_ids : [page.secondary_keywords_ids]) : []
            
            // Por cada secondary, primero intentar por ID exacto
            for (const secId of secondaryIdTexts) {
              if (secId && validKeywordIds.has(secId)) {
                secondaryIds.push(secId)
              }
            }
            // Buscar por texto los que no se encontraron por ID - matching flexible
            for (const secText of secondaryTexts) {
              if (!secText) continue
              const secTextLower = String(secText).toLowerCase().trim()
              // Primero buscar coincidencia exacta
              let match = keywords.find(k => 
                k.keyword.toLowerCase().trim() === secTextLower
              )
              // Si no hay coincidencia exacta, buscar por inclusiÃ³n
              if (!match) {
                match = keywords.find(k => 
                  k.keyword.toLowerCase().includes(secTextLower) ||
                  secTextLower.includes(k.keyword.toLowerCase())
                )
              }
              if (match && !secondaryIds.includes(match.id)) {
                secondaryIds.push(match.id)
              } else if (!match) {
                console.log(`Secondary keyword no encontrada: "${secText}"`)
              }
            }
            
            // Obtener el texto de la keyword principal para intenciones
            const mainKw = keywords.find(k => k.id === keywordId)
            if (mainKw) {
              intentions[mainKw.keyword] = page.intent || 'informational'
            }
            
            convertedPages.push({
              main_keyword: mainKw?.keyword || mainKeywordText || '',
              main_keyword_id: keywordId,
              secondary_keywords: secondaryIds.map(id => {
                const kw = keywords.find(k => k.id === id)
                return kw?.keyword || ''
              }).filter(k => k),
              secondary_keywords_ids: secondaryIds,
              type: page.type,
              is_pillar: page.is_pillar,
              intent: page.intent
            })
          }
          
          if (convertedPages.length > 0) {
            convertedCategories.push({
              name: cat.name,
              pages: convertedPages
            })
          }
        }
        
        if (convertedCategories.length > 0) {
          convertedSilos.push({
            name: silo.name,
            categories: convertedCategories
          })
        }
      }
      
      return { silos: convertedSilos, intentions }
    }

    const { silos: validatedSilos, intentions } = validateAndConvert(parsed.silos)
    
    console.log('ValidaciÃ³n de keywords:', validationErrors.length > 0 ? validationErrors.join(', ') : 'OK')

    return {
      silos: validatedSilos,
      intentions,
      validationErrors
    }

  } catch (error: any) {
    if (attempt <= MAX_RETRIES) {
      console.log(`Retrying SILO analysis (attempt ${attempt + 1})...`)
      await new Promise(resolve => setTimeout(resolve, 2000))
      return analyzeSilosWithGemini(keywords, apiKey, existingSilos, context, aiModel, aiParams, attempt + 1)
    }
    throw error
  }
}

export async function POST(request: NextRequest) {
  const supabase = createClient(supabaseUrl, supabaseServiceKey)
  try {
    const body = await request.json()
    const { keywordIds, useExistingSilos } = body
    
    console.log('=== API DEBUG ===')
    console.log('Full body:', body)
    console.log('keywordIds type:', typeof keywordIds)
    console.log('keywordIds isArray:', Array.isArray(keywordIds))
    console.log('keywordIds received:', keywordIds?.length, keywordIds)
    console.log('useExistingSilos:', useExistingSilos)
    
    const apiKey = process.env.GOOGLE_AI_API_KEY
    console.log('GOOGLE_AI_API_KEY exists:', !!apiKey)
    
    if (!apiKey) {
      console.log('ERROR: No API key')
      return NextResponse.json({ error: 'API key no configurada' }, { status: 500 })
    }

    let keywords: { id: string; keyword: string }[] = []
    
    if (keywordIds && keywordIds.length > 0) {
      console.log('Fetching keywords with IDs...')
      console.log('First 3 keywordIds:', keywordIds.slice(0, 3))
      console.log('keywordIds type:', typeof keywordIds[0])
      
      // Fetch in batches if too many IDs
      let allKeywords: { id: string; keyword: string }[] = []
    const BATCH_SIZE = 40
      
      for (let i = 0; i < keywordIds.length; i += BATCH_SIZE) {
        const batch = keywordIds.slice(i, i + BATCH_SIZE)
        const { data, error } = await supabase
          .from('d_seo_admin_raw_keywords')
          .select('id, keyword')
          .in('id', batch)
        
        if (error) {
          console.log('Batch query error:', error)
        }
        if (data) {
          allKeywords = [...allKeywords, ...data]
        }
      }
      
      console.log('Query result - total data:', allKeywords.length)
      if (allKeywords.length > 0) {
        console.log('Sample returned:', allKeywords.slice(0, 2))
      }
      keywords = allKeywords
    } else {
      console.log('Fetching all pending keywords...')
      const { data, error } = await supabase
        .from('d_seo_admin_raw_keywords')
        .select('id, keyword')
        .eq('status', 'pending')
        .limit(1000)
      
      console.log('Query result - data:', data?.length, 'error:', error)
      keywords = data || []
    }

    console.log('Keywords found:', keywords.length)
    
    if (keywords.length === 0) {
      console.log('ERROR: No keywords found - returning detailed error')
      return NextResponse.json({ 
        error: 'No hay keywords pendientes para analizar',
        debug: {
          keywordIdsReceived: keywordIds?.length || 0,
          useExistingSilos
        }
      }, { status: 400 })
    }

    const existingSilos: SiloForPrompt[] = []
    
    const context = await getCompanyContext()
    console.log('Company context loaded:', context ? 'yes' : 'no')
    
    if (useExistingSilos) {
      const { data: silos } = await supabase
        .from('d_seo_admin_silos')
        .select('id, name')

      if (silos && silos.length > 0) {
        for (const silo of silos) {
          const { data: cats } = await supabase
            .from('d_seo_admin_categories')
            .select('id, name')
            .eq('silo_id', silo.id)
          
          const categories = []
          if (cats && cats.length > 0) {
            for (const cat of cats) {
              const { data: pages } = await supabase
                .from('d_seo_admin_pages')
                .select('main_keyword')
                .eq('category_id', cat.id)
              
              categories.push({
                name: cat.name,
                pages: pages || []
              })
            }
          }
          existingSilos.push({
            name: silo.name,
            categories
          })
        }
      }
    }

    const aiConfig = await getAIConfig()
    const siloConfig = aiConfig?.silo || { model: 'gemini-2.5-pro', parameters: { maxTokens: 20000, temperature: 0.3 } }
    const aiModel = siloConfig.model
    const aiParams = siloConfig.parameters || {}

    const BATCH_SIZE = 40
    const allSilos: any[] = []
    const allIntentions: { [key: string]: string } = {}
    const allValidationErrors: string[] = []
    const processedKeywordIds: string[] = []
    
    // Process keywords in batches
    for (let i = 0; i < keywords.length; i += BATCH_SIZE) {
      const batchKeywords = keywords.slice(i, i + BATCH_SIZE)
      processedKeywordIds.push(...batchKeywords.map(k => k.id))
      
      console.log(`Analyzing batch ${Math.floor(i / BATCH_SIZE) + 1}: ${batchKeywords.length} keywords with model: ${aiModel}`)
      
      // Convert allSilos to prompt format if needed
      const silosForPrompt: SiloForPrompt[] = useExistingSilos 
        ? existingSilos 
        : (allSilos as any[]).map((silo: any) => ({
            name: silo.name,
            categories: (silo.categories || []).map((cat: any) => ({
              name: cat.name,
              pages: (cat.pages || []).map((page: any) => ({
                main_keyword: page.main_keyword || ''
              }))
            }))
          }))
      
      // Use existing silos from previous batches to avoid duplicates
      const proposal = await analyzeSilosWithGemini(
        batchKeywords, 
        apiKey, 
        silosForPrompt, 
        context || {}, 
        aiModel, 
        aiParams
      )
      
      // Merge results, avoiding duplicate silo names
      for (const silo of proposal.silos) {
        if (!silo || !silo.name) continue
        const existingSilo = allSilos.find((s: any) => s.name?.toLowerCase() === silo.name?.toLowerCase())
        if (!existingSilo) {
          allSilos.push(silo)
        } else {
          // Merge categories into existing silo
          for (const cat of (silo.categories || [])) {
            if (!cat || !cat.name) continue
            const existingCat = existingSilo.categories.find((c: any) => c.name?.toLowerCase() === cat.name?.toLowerCase())
            if (!existingCat) {
              existingSilo.categories.push(cat)
            } else {
              // Merge pages into existing category
              for (const page of (cat.pages || [])) {
                if (!page || !page.main_keyword_id) continue
                if (!existingCat.pages.find((p: any) => p.main_keyword_id === page.main_keyword_id)) {
                  existingCat.pages.push(page)
                }
              }
            }
          }
        }
      }
      
      // Merge intentions and errors
      Object.assign(allIntentions, proposal.intentions)
      allValidationErrors.push(...proposal.validationErrors)
    }

    return NextResponse.json({
      success: true,
      keywordCount: keywords.length,
      keywordsAnalyzed: processedKeywordIds.length,
      keywords: processedKeywordIds.map((id, i) => ({ id, keyword: keywords[i].keyword })),
      proposal: allSilos,
      intentions: allIntentions,
      validationErrors: allValidationErrors,
      existingSilosUsed: useExistingSilos && existingSilos.length > 0,
      existingSilosCount: existingSilos.length,
      contextUsed: !!context,
      batchesProcessed: Math.ceil(keywords.length / BATCH_SIZE)
    })

  } catch (error: any) {
    console.error('Error in analyze-proposal:', error)
    return NextResponse.json({ 
      error: error.message || 'Error analyzing keywords',
      details: error.toString()
    }, { status: 500 })
  }
}
